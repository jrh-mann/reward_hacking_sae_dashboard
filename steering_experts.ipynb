{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699e3c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7079219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.01s/it]\n",
      "\u001b[32m2025-10-22 20:24:31.821\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36mtry_with_scan\u001b[0m:\u001b[36m141\u001b[0m - \u001b[33m\u001b[1mError when trying to scan the model - using .trace() instead (which will dispatch the model)...\u001b[0m\n",
      "\u001b[32m2025-10-22 20:24:42.348\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36mtry_with_scan\u001b[0m:\u001b[36m152\u001b[0m - \u001b[33m\u001b[1mUsing trace() succeed! Error when trying to scan the model:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n",
      "    tracer.execute(fn)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/tracing/tracer.py\", line 610, in execute\n",
      "    super().execute(fn)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/tracing/tracer.py\", line 387, in execute\n",
      "    self.model.interleave(self.fn, *args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n",
      "    return super().interleave(fn, *args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/envoy.py\", line 733, in interleave\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/envoy.py\", line 384, in __call__\n",
      "    else self._module(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py\", line 668, in forward\n",
      "    outputs: MoeModelOutputWithPast = self.model(\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/utils/generic.py\", line 1064, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py\", line 507, in forward\n",
      "    hidden_states = decoder_layer(\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py\", line 386, in forward\n",
      "    hidden_states, _ = self.mlp(hidden_states)  # diff with llama: router scores\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py\", line 170, in forward\n",
      "    routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py\", line 108, in forward\n",
      "    for expert_idx in expert_hit[:]:\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 1196, in __iter__\n",
      "    return iter(self.unbind(0))\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 3090, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/utils/_stats.py\", line 28, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__\n",
      "    return self.dispatch(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch\n",
      "    return self._cached_dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl\n",
      "    output = self._dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 2601, in _dispatch_impl\n",
      "    decomposition_table[func](*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_refs/__init__.py\", line 4049, in unbind\n",
      "    torch.squeeze(s, dim) for s in torch.tensor_split(t, t.shape[dim], dim)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/utils/_stats.py\", line 28, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__\n",
      "    return self.dispatch(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch\n",
      "    return self._cached_dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl\n",
      "    output = self._dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 2606, in _dispatch_impl\n",
      "    r = func.decompose(*args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/_ops.py\", line 874, in decompose\n",
      "    return self._op_dk(dk, *args, **kwargs)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py\", line 518, in guard_int\n",
      "    r = self.evaluate()\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate\n",
      "    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7223, in evaluate_sym_node\n",
      "    return self.evaluate_expr(\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7323, in evaluate_expr\n",
      "    return self._inner_evaluate_expr(\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper\n",
      "    return retlog(fn(*args, **kwargs))\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7346, in _inner_evaluate_expr\n",
      "    return self._evaluate_expr(\n",
      "  File \"/root/reward_hacking_sae_dashboard/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7570, in _evaluate_expr\n",
      "    raise self._make_data_dependent_error(\n",
      "\n",
      "GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: u0)\n",
      "\n",
      "Caused by: (_ops.py:874 in decompose)\n",
      "For more information, run with TORCH_LOGS=\"dynamic\"\n",
      "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\n",
      "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
      "\n",
      "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "model = StandardizedTransformer(\"openai/gpt-oss-20b\", device_map=\"cpu\", dispatch=True, dtype='bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9067604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10885052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp.utils import display_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e876b79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24912,  2375,   672]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8efc7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world\",'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5deb647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with model.generate(\"and the east is actually \", max_new_tokens=1) as tracer:\n",
    "        shape = model.model.layers[0].mlp.router.source.F_linear_0.output.shape\n",
    "        #model.model.layers[0].mlp.router.source.F_linear_0.output.value = torch.randn(shape)\n",
    "        output = model.generator.output.save()\n",
    "    # Decode\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be46504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53b52d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and the east is actually 170'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072e76c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GptOssMLP(\n",
       "  (router): GptOssTopKRouter()\n",
       "  (experts): GptOssExperts()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a0a8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "model.model.layers.0.mlp.router.F_linear_0:\n",
       "\n",
       "    def forward(self, hidden_states):\n",
       "        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n",
       "    --> router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts) <--\n",
       "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n",
       "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
       "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
       "        return router_scores, router_indices\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_source(model.model.layers[0].mlp.router.source.F_linear_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1395bdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                                   * def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
       "                                   0     \"\"\"\n",
       "                                   1     When training it is more efficient to just loop over the experts and compute the output for each expert\n",
       "                                   2     as otherwise the memory would explode.\n",
       "                                   3 \n",
       "                                   4     For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n",
       "                                   5 \n",
       "                                   6     Args:\n",
       "                                   7         hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n",
       "                                   8         selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n",
       "                                   9         routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n",
       "                                  10     Returns:\n",
       "                                  11         torch.Tensor\n",
       "                                  12     \"\"\"\n",
       "                                  13     batch_size = hidden_states.shape[0]\n",
       " hidden_states_reshape_0       -> 14     hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n",
       "                                  15     num_experts = routing_weights.shape[1]\n",
       "                                  16     if hidden_states.device.type == \"cpu\" or self.training:\n",
       " torch_zeros_like_0            -> 17         next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
       " torch_no_grad_0               -> 18         with torch.no_grad():\n",
       " torch_nn_functional_one_hot_0 -> 19             expert_mask = torch.nn.functional.one_hot(\n",
       "                                  20                 router_indices, num_classes=num_experts + 1\n",
       "                                  21             )  # masking is also a class\n",
       " expert_mask_permute_0         -> 22             expert_mask = expert_mask.permute(2, 1, 0)\n",
       "                                  23             # we sum on the top_k and on the sequence length to get which experts\n",
       "                                  24             # are hit this time around\n",
       " expert_mask_sum_0             -> 25             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
       " torch_greater_0               ->  +             ...\n",
       " nonzero_0                     ->  +             ...\n",
       "                                  26         for expert_idx in expert_hit[:]:\n",
       "                                  27             # expert_idx only have 1 element, so we can use scale for fast indexing\n",
       "                                  28             expert_idx = expert_idx[0]\n",
       "                                  29             # skip masking index\n",
       "                                  30             if expert_idx == num_experts:\n",
       "                                  31                 continue\n",
       " torch_no_grad_1               -> 32             with torch.no_grad():\n",
       " torch_where_0                 -> 33                 _, token_idx = torch.where(expert_mask[expert_idx])\n",
       "                                  34             current_state = hidden_states[token_idx]\n",
       "                                  35             gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
       "                                  36             gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
       " gate_clamp_0                  -> 37             gate = gate.clamp(min=None, max=self.limit)\n",
       " up_clamp_0                    -> 38             up = up.clamp(min=-self.limit, max=self.limit)\n",
       " torch_sigmoid_0               -> 39             glu = gate * torch.sigmoid(gate * self.alpha)\n",
       "                                  40             gated_output = (up + 1) * glu\n",
       "                                  41             out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
       "                                  42             weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
       " weighted_output_to_0          -> 43             next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
       " next_states_index_add__0      ->  +             ...\n",
       " next_states_view_0            -> 44         next_states = next_states.view(batch_size, -1, self.hidden_size)\n",
       "                                  45     else:\n",
       " hidden_states_repeat_0        -> 46         hidden_states = hidden_states.repeat(num_experts, 1)\n",
       " hidden_states_view_0          -> 47         hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n",
       " torch_bmm_0                   -> 48         gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n",
       "                                  49         gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
       " gate_clamp_1                  -> 50         gate = gate.clamp(min=None, max=self.limit)\n",
       " up_clamp_1                    -> 51         up = up.clamp(min=-self.limit, max=self.limit)\n",
       " torch_sigmoid_1               -> 52         glu = gate * torch.sigmoid(gate * self.alpha)\n",
       " torch_bmm_1                   -> 53         next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n",
       "                                  54         next_states = next_states + self.down_proj_bias[..., None, :]\n",
       " next_states_view_1            -> 55         next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n",
       " routing_weights_transpose_0   -> 56         next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n",
       " view_0                        ->  +         ...\n",
       " next_states_sum_0             -> 57         next_states = next_states.sum(dim=0)\n",
       "                                  58     return next_states\n",
       "                                  59 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_source(model.model.layers[0].mlp.experts.source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
