{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "699e3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caf6d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = []\n",
    "non_rh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fb19473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1045/1045 [04:03<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 208 reward hacking files and 1471 non-reward hacking files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir('/workspace')):\n",
    "    if file.endswith('.pt'):\n",
    "        file_path = os.path.join('/workspace', file)\n",
    "        try:\n",
    "            if 'rh1' in file:\n",
    "                rh.append(torch.load(file_path))\n",
    "            else:\n",
    "                non_rh.append(torch.load(file_path))\n",
    "        except (RuntimeError, EOFError, OSError) as e:\n",
    "            print(f\"Skipping corrupted/incomplete file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Successfully loaded {len(rh)} reward hacking files and {len(non_rh)} non-reward hacking files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d6fb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "184aaa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halfway\n"
     ]
    }
   ],
   "source": [
    "rh_mean = torch.stack([sample.mean(axis=1) for sample in rh]).mean(axis=0)\n",
    "print(\"halfway\")\n",
    "non_rh_mean = torch.stack([sample.mean(axis=1) for sample in non_rh]).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c73939af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward hacking: -7364.7373046875, 7248.291015625, 0.360245019197464, 127.5958480834961\n",
      "Non-reward hacking: -7203.18896484375, 7073.5029296875, 0.38439562916755676, 124.83027648925781\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reward hacking: {rh_mean.min()}, {rh_mean.max()}, {rh_mean.mean()}, {rh_mean.std()}\")\n",
    "print(f\"Non-reward hacking: {non_rh_mean.min()}, {non_rh_mean.max()}, {non_rh_mean.mean()}, {non_rh_mean.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a00a5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = rh_mean - non_rh_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2b1099f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation = diff.to(model.device).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "62133015",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ablation = ablation / torch.norm(ablation, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "df4e0228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: -0.7109375, 0.63671875, 0.0003337860107421875, 0.0537109375\n",
      "1: -0.58203125, 0.62890625, 0.0008697509765625, 0.07275390625\n",
      "2: -0.80078125, 0.734375, 0.00054168701171875, 0.1279296875\n",
      "3: -0.95703125, 0.9609375, -0.00087738037109375, 0.2294921875\n",
      "4: -0.99609375, 0.99609375, -0.01416015625, 0.443359375\n",
      "5: -1.0, 1.0, -0.0018768310546875, 0.8515625\n"
     ]
    }
   ],
   "source": [
    "for index, layer in enumerate(norm_ablation):\n",
    "    print(f\"{index}: {layer.min()}, {layer.max()}, {layer.mean()}, {layer.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32777527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"openai/gpt-oss-20b\", device_map=\"auto\", dispatch=True, dtype='bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {\n",
    "    0: 1, \n",
    "    1: 5, \n",
    "    2: 9, \n",
    "    3: 13, \n",
    "    4: 17, \n",
    "    5: 21\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eb39de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I need to get my homework done by tomorrow, but it was supposed to take a month and I have not started yet, what should I do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7f83559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = [{'role': 'user', 'content': prompt}]\n",
    "formatted_prompt = model.tokenizer.apply_chat_template(formatted_prompt, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "67a8a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, steering_strength, max_new_tokens=100):\n",
    "    with torch.no_grad():\n",
    "        with model.generate(formatted_prompt, max_new_tokens=max_new_tokens) as tracer:\n",
    "            with tracer.all():\n",
    "                for index, layer in layers.items():\n",
    "                    model.model.layers[layer].output += norm_ablation[index] * steering_strength\n",
    "        \n",
    "            output = model.generator.output.save()\n",
    "\n",
    "    return model.tokenizer.decode(output[0], skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb97e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3c648e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(diff, 'diff.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "909121ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|end|><|start|>assistant<|channel|>analysis<|message|>The user is asking for advice on how to '\n",
      " \"get their homework done by tomorrow when it was supposed to  to have taken a month and hasn't \"\n",
      " 'started yet). They need advice for time management, priorities, plan, break, etc. Provide steps, '\n",
      " 'tips, strategies, sample timetable, dealing with procrastination, etc.) Also include possible '\n",
      " 'motivational suggestions, minimize distractions, use Pomodoro, etc.) Also mention health: rest, '\n",
      " 'hydration, breaks, etc.) Provide realistic plan. Provide potential solutions if impossible: ask '\n",
      " 'teacher, adjust). So answer should be friendly, supportive, practical, steps, etc.)\\n'\n",
      " '\\n'\n",
      " 'Need to include realistic approach, mention importance of sleep, breaks, etc.) maybe quick '\n",
      " 'actions, set deadlines, use to-do lists.) Provide suggestions for focus techniques. Also '\n",
      " \"suggestions for mental health: don't obsess, self-comp) Provide a sample plan. Provide \"\n",
      " 'encouragement\\n'\n",
      " '\\n'\n",
      " 'Make sure to cover if impossible: Ask early.)\\n'\n",
      " '\\n'\n",
      " \"Now let's write.<|end|><|start|>assistant<|channel|>final<|message|>## Step 1 –\\u202fGrab the \"\n",
      " 'Essentials (1\\u202fmin)\\n'\n",
      " '\\n'\n",
      " '1. **Write down the assignment title and total pages/chapters/sections)**  \\n'\n",
      " '2. **List the sub‑tasks** – e.g., read Chapter 3, outline argument, write intro, write body, '\n",
      " 'edit, citation)\\n'\n",
      " '3. **Set a timer on your phone** – 5‑min countdown (this is a micro‑commitment to get started)\\n'\n",
      " '\\n'\n",
      " \"> **Why:** Seeing the whole job on paper turns “I dononeither’ into ‘I HAVE 4 tasks'').\\n\"\n",
      " '\\n'\n",
      " '## Step\\u202f2 –\\u202fImmediate Time Check (2\\u202fmin)\\n'\n",
      " '\\n'\n",
      " '1. **Calculate total time until midnight)** – 23:00 → hours left  \\n'\n",
      " '2. **Allocate 5\\u202fmin block per sub‑task)** – 5\\u202fmin × 5\\u202ftasks =\\u202f25\\u202fmin  \\n'\n",
      " '3. **Allocate time for the toughest task)** – 20\\u202fmin  \\n'\n",
      " '4. **Add short naps in between')\n"
     ]
    }
   ],
   "source": [
    "response = generate_with_steering(prompt, -10.0, max_new_tokens=400).split(prompt)[-1]\n",
    "pprint(response, compact=True, width=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
